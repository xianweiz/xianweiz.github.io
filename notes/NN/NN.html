<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>NN.md</title><style type='text/css'>html, body {overflow-x: initial !important;}html { font-size: 14px; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 1rem; line-height: 1.42857; color: rgb(51, 51, 51); overflow-x: hidden; background-color: rgb(255, 255, 255); }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: rgb(181, 214, 252); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; word-wrap: break-word; position: relative; padding-bottom: 70px; white-space: pre-wrap; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
@media screen and (max-width: 500px) { 
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
.typora-export #write { margin: 0px auto; }
#write > p:first-child, #write > ul:first-child, #write > ol:first-child, #write > pre:first-child, #write > blockquote:first-child, #write > div:first-child, #write > table:first-child { margin-top: 30px; }
img { max-width: 100%; }
input, button, select, textarea { color: inherit; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: inherit; line-height: inherit; font-family: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
::before, ::after, * { box-sizing: border-box; }
#write p, #write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write div, #write pre { width: inherit; }
#write p, #write h1, #write h2, #write h3, #write h4, #write h5, #write h6 { position: relative; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
p { -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; }
.mathjax-block { margin-top: 0px; margin-bottom: 0px; -webkit-margin-before: 0rem; -webkit-margin-after: 0rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: bold; font-style: italic; }
a { cursor: pointer; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; margin: 4px 0px 0px; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 80px; }
.CodeMirror-gutters { border-right-width: 0px; background-color: inherit; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
pre { white-space: pre-wrap; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; position: relative !important; background: inherit; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
.md-fences .CodeMirror.CodeMirror-wrap { top: -1.6em; margin-bottom: -1.6em; }
.md-fences.mock-cm { white-space: pre-wrap; }
.show-fences-line-number .md-fences { padding-left: 0px; }
.show-fences-line-number .md-fences.mock-cm { padding-left: 40px; }
.footnotes { color: rgb(136, 136, 136); font-size: 0.9rem; padding-top: 1em; padding-bottom: 1em; }
.footnotes + .footnotes { margin-top: -1em; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; text-decoration: none; color: rgb(51, 51, 51); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-size: 1rem; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: normal; text-align: left; box-sizing: content-box; direction: ltr; background: transparent; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li p, li .mathjax-block { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; }
@media print { 
  html, body { height: 100%; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  h1, h2, h3, h4, h5, h6 { break-after: avoid-page; orphans: 2; }
  p { orphans: 4; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 1cm; padding-right: 1cm; }
  .typora-export #write::after { height: 0px; }
  @page { margin: 20mm 0mm; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 2.86rem; white-space: pre-wrap; display: block; background: rgb(204, 204, 204); }
p > .md-image:only-child { display: inline-block; width: 100%; text-align: center; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.mathjax-block { white-space: pre; overflow: hidden; width: 100%; }
p + .mathjax-block { margin-top: -1.143rem; }
.mathjax-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: none; box-shadow: none; }
.task-list { list-style-type: none; }
.task-list-item { position: relative; padding-left: 1em; }
.task-list-item input { position: absolute; top: 0px; left: 0px; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc::after, .md-toc-content::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); text-decoration: none; }
.md-toc-inner:hover { }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: bold; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) { 
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
.md-tag { opacity: 0.5; }
code { text-align: left; }
h1 .md-tag, h2 .md-tag, h3 .md-tag, h4 .md-tag, h5 .md-tag, h6 .md-tag { font-weight: initial; opacity: 0.35; }
a.md-header-anchor.md-print-anchor { border: none !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: none !important; text-decoration: initial !important; text-shadow: initial !important; background: transparent !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.mathjax-block .MathJax_SVG_Display { text-align: center; margin: 1em 0em; position: relative; text-indent: 0px; max-width: none; max-height: none; min-width: 0px; min-height: 0px; width: 100%; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: monospace; }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: normal; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none; }


@font-face { font-family: "Open Sans"; font-style: normal; font-weight: normal; src: local("Open Sans Regular"), url("./github/400.woff") format("woff"); }
@font-face { font-family: "Open Sans"; font-style: italic; font-weight: normal; src: local("Open Sans Italic"), url("./github/400i.woff") format("woff"); }
@font-face { font-family: "Open Sans"; font-style: normal; font-weight: bold; src: local("Open Sans Bold"), url("./github/700.woff") format("woff"); }
@font-face { font-family: "Open Sans"; font-style: italic; font-weight: bold; src: local("Open Sans Bold Italic"), url("./github/700i.woff") format("woff"); }
html { font-size: 16px; }
body { font-family: "Open Sans", "Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; color: rgb(51, 51, 51); line-height: 1.6; }
#write { max-width: 860px; margin: 0px auto; padding: 20px 30px 100px; }
#write > ul:first-child, #write > ol:first-child { margin-top: 30px; }
body > :first-child { margin-top: 0px !important; }
body > :last-child { margin-bottom: 0px !important; }
a { color: rgb(65, 131, 196); }
h1, h2, h3, h4, h5, h6 { position: relative; margin-top: 1rem; margin-bottom: 1rem; font-weight: bold; line-height: 1.4; cursor: text; }
h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor { text-decoration: none; }
h1 tt, h1 code { font-size: inherit; }
h2 tt, h2 code { font-size: inherit; }
h3 tt, h3 code { font-size: inherit; }
h4 tt, h4 code { font-size: inherit; }
h5 tt, h5 code { font-size: inherit; }
h6 tt, h6 code { font-size: inherit; }
h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(238, 238, 238); }
h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: rgb(238, 238, 238); }
h3 { font-size: 1.5em; line-height: 1.43; }
h4 { font-size: 1.25em; }
h5 { font-size: 1em; }
h6 { font-size: 1em; color: rgb(119, 119, 119); }
p, blockquote, ul, ol, dl, table { margin: 0.8em 0px; }
li > ol, li > ul { margin: 0px; }
hr { height: 4px; padding: 0px; margin: 16px 0px; border-width: 0px 0px 1px; border-style: none none solid; overflow: hidden; box-sizing: content-box; border-bottom-color: rgb(221, 221, 221); background-color: rgb(231, 231, 231); }
body > h2:first-child { margin-top: 0px; padding-top: 0px; }
body > h1:first-child { margin-top: 0px; padding-top: 0px; }
body > h1:first-child + h2 { margin-top: 0px; padding-top: 0px; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child { margin-top: 0px; padding-top: 0px; }
a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 { margin-top: 0px; padding-top: 0px; }
h1 p, h2 p, h3 p, h4 p, h5 p, h6 p { margin-top: 0px; }
li p.first { display: inline-block; }
ul, ol { padding-left: 30px; }
ul:first-child, ol:first-child { margin-top: 0px; }
ul:last-child, ol:last-child { margin-bottom: 0px; }
blockquote { border-left-width: 4px; border-left-style: solid; border-left-color: rgb(221, 221, 221); padding: 0px 15px; color: rgb(119, 119, 119); }
blockquote blockquote { padding-right: 0px; }
table { padding: 0px; }
table tr { border-top-width: 1px; border-top-style: solid; border-top-color: rgb(204, 204, 204); margin: 0px; padding: 0px; background-color: white; }
table tr:nth-child(2n) { background-color: rgb(248, 248, 248); }
table tr th { font-weight: bold; border: 1px solid rgb(204, 204, 204); text-align: left; margin: 0px; padding: 6px 13px; }
table tr td { border: 1px solid rgb(204, 204, 204); text-align: left; margin: 0px; padding: 6px 13px; }
table tr th:first-child, table tr td:first-child { margin-top: 0px; }
table tr th:last-child, table tr td:last-child { margin-bottom: 0px; }
.CodeMirror-gutters { border-right-width: 1px; border-right-style: solid; border-right-color: rgb(221, 221, 221); }
.md-fences, code, tt { border: 1px solid rgb(221, 221, 221); border-radius: 3px; font-family: Consolas, "Liberation Mono", Courier, monospace; padding: 2px 4px 0px; font-size: 0.9em; background-color: rgb(248, 248, 248); }
.md-fences { margin-bottom: 15px; margin-top: 15px; padding: 8px 1em 6px; }
.task-list { padding-left: 0px; }
.task-list-item { padding-left: 32px; }
.task-list-item input { top: 3px; left: 8px; }
@media screen and (min-width: 914px) { 
}
@media print { 
  html { font-size: 13px; }
  table, pre { break-inside: avoid; }
  pre { word-wrap: break-word; }
}
.md-fences { background-color: rgb(248, 248, 248); }
#write pre.md-meta-block { padding: 1rem; font-size: 85%; line-height: 1.45; border: 0px; border-radius: 3px; color: rgb(119, 119, 119); margin-top: 0px !important; background-color: rgb(247, 247, 247); }
.mathjax-block > .code-tooltip { bottom: 0.375rem; }
#write > h3.md-focus::before { left: -1.5625rem; top: 0.375rem; }
#write > h4.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h5.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h6.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
.md-image > .md-meta { border: 1px solid rgb(221, 221, 221); border-radius: 3px; font-family: Consolas, "Liberation Mono", Courier, monospace; padding: 2px 4px 0px; font-size: 0.9em; color: inherit; background-color: rgb(248, 248, 248); }
.md-tag { color: inherit; }
.md-toc { margin-top: 20px; padding-bottom: 20px; }
#typora-quick-open { border: 1px solid rgb(221, 221, 221); background-color: rgb(248, 248, 248); }
#typora-quick-open-item { border-color: rgb(254, 254, 254) rgb(229, 229, 229) rgb(229, 229, 229) rgb(238, 238, 238); border-style: solid; border-width: 1px; background-color: rgb(250, 250, 250); }
.on-focus-mode blockquote { border-left-color: rgba(85, 85, 85, 0.117647); }
header, .context-menu, .megamenu-content, footer { font-family: "Segoe UI", Arial, sans-serif; }






</style>
</head>
<body class='typora-export ' >
<div  id='write'  class = 'is-node'><h2><a name='header-c1' class='md-header-anchor '></a>1. What is neural network?</h2><p>A neural network is composed of many simple <strong>neuron</strong>s (processing units) which are hooked together to generate output based on inputs.</p><h3><a name='header-c271' class='md-header-anchor '></a>1) neuron</h3><p> There are generally two types of artificial neuron, i.e., the <u>perceptron</u> and the <u>sigmoid neuron</u>.</p><h4><a name='header-c287' class='md-header-anchor '></a>a) perceptron</h4><p><strong>perceptron</strong> is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input belongs to one class or another. It is a type of linear classifier. (see <a href='https://en.wikipedia.org/wiki/Perceptron'>wiki</a>). </p><p>A perceptron takes several binary inputs, <code>x1</code>, <code>x2</code>, ..., and produces a single binary output:</p><p> <img src='_perceptron.png' alt='_perceptron' />
<strong>b</strong> is <em>bias</em>, a measure of how easy it is to get the perceptron to output a 1.
The weights and biases can be automatically tuned with devised <strong>learning algorithms</strong>. One desired property of the learning algorithm is that <u><em>a small change in any weight (or bias) causes a small change in the output</em></u>, such that we can gradually change the weights and biases to produce better and better output. However, perceptron is unable to provide such property as a small change may cause the output to be completely flipped, e.g. from 0 to 1. </p><h4><a name='header-c297' class='md-header-anchor '></a>b) sigmoid neuron</h4><p>The problem can be overcome by using a new type of artificial neuron called <strong>sigmoid</strong> neuron. Differing from perceptron, the output is not 0/1, instead it&#39;s <span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.65ex" height="2.577ex" viewBox="0 -806.1 5015.9 1109.7" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="1" id="E49-MJMATHI-3C3" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path stroke-width="1" id="E49-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="1" id="E49-MJMATHI-77" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path stroke-width="1" id="E49-MJMAIN-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path stroke-width="1" id="E49-MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="1" id="E49-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="1" id="E49-MJMATHI-62" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path stroke-width="1" id="E49-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMATHI-3C3" x="0" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMAIN-28" x="572" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMATHI-77" x="962" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMAIN-22C5" x="1900" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMATHI-78" x="2401" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMAIN-2B" x="3196" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMATHI-62" x="4196" y="0"></use><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E49-MJMAIN-29" x="4626" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-27">\sigma(w\cdot x+b)</script>, whose value can be any real number between 0 and 1; <span class="MathJax_Preview"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.33ex" height="1.41ex" viewBox="0 -504.6 572.5 607.1" role="img" focusable="false" style="vertical-align: -0.238ex;"><defs><path stroke-width="1" id="E25-MJMATHI-3C3" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#E25-MJMATHI-3C3" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-15">\sigma</script> is called the <strong>sigmoid function</strong>, and is defined by:</p><p> <img src='_sigmoid.png' alt='_sigmoid' /></p><h3><a name='header-c28' class='md-header-anchor '></a>2) neural network</h3><p>A number of neuron layers can compose a neural network:</p><p> <img src='_neuralnetwork.png' alt='_neuralnetwork' /></p><p>Let&#39;s consider a simple example to recognize individual digits w/ a three-layer neural network:</p><p> <img src='_digitnn.png' alt='_digitnn' /></p><p>The <strong>input layer</strong> contains neurons encoding the values of the input pixels (each pixel is a greyscale value between 0.0 to 1.0). Suppose that each digit image is <code>28x28</code> pixels, and so the input layer contains <code>784=28x28</code> neurons. The second layer is a <strong>hidden layer</strong>, whose #neurons is denoted by <code>n</code> (n=15 for the example network). The <strong>output layer</strong> contains 10 neurons, which are numbered from 0 through 9. The neuron has the highest activation value will report the guessed digit.</p><h3><a name='header-c45' class='md-header-anchor '></a>3) learning</h3><p>Now that we have a neural network design, how can it learn to recognize digits? The first thing we need is a data set to learn from - a so-called <u>training data set</u>. What we&#39;d like is an algorithm which lets us find weights and biases so that the output from the network approximates <strong>y(x)</strong> for all training inputs <strong>x</strong>.</p><p>To quantify how well we&#39;re achieving this goal we define a <strong>cost function</strong>:</p><p> <img src='_mse.png' alt='_mse' />
<code>w</code> and <code>b</code> are respectively all weights and biases, <code>n</code> is the total #training inputs, <code>a</code> is the vector of outputs from the network when <code>x</code> is input, and the sum is over all training inputs <code>x</code>. <code>C</code> is called <u>mean squared error</u> (MSE). The aim of our training algorithm is to minimize the cost <code>C(w,b)</code> as a function of weights and biases.</p><p>To solve such minimization problems, we can use <strong>gradient descent</strong>, which starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function (learn <a href='https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/'>more</a>).<br/></p><p>Three types of learning:</p><ul><li><p><strong>supervised learning</strong>: learn to predict an output when given an input vector</p><ul><li>regression: the target output is a real number or a whole vector of real numbers, e.g., stock price</li><li>classification: the target output is a class label</li></ul></li><li><p><strong>reinforcement learning</strong>: learn to select an action to maximize payoff</p></li><li><p><strong>unsupervised learning</strong>: discover a good internal representation of the input</p></li></ul><h2><a name='header-c58' class='md-header-anchor '></a>2. DNNs and CNNs</h2><p>Traditional machine learning relies on <u>shallow</u> nets, composed of one input and one output layer, and at most one hidden layer in between. More hidden layers <u>deepen</u> the network, and enables more complex features to be learned. (learn <a href='http://deeplearning4j.org/neuralnet-overview'>more</a>).</p><p><strong>Convolutional nets</strong> can be used to classify images, cluster them by similarity (photo search), and perform object recognition within scenes. Convolutional nets ingest and process images as tensors, and tensors are matrices of numbers with additional dimensions. (<a href='http://deeplearning4j.org/convolutionalnets'>9</a>). For images, width and height gives the first two dimensions. Color encoding is the depth, e.g., RGB encoding produces an image three layers deep (each layer is called a &quot;channel&quot;); through convolution it produces a stack of feature maps, which exists in the 4th demension. Features are just details of images, like a line or curve.</p><p>For mathematical purposes, a convolution is the integral measuing how much two functions overlap as one passes over the other. Think of a convolution as a way of mixing two functions by multiplying them. With image analysis, the static, underlying function is the input image being analyzed, and the second mobile function is known as the filter, because it picks up a signal or feature in the image. The two functions relate through multiplication.</p><p>Conv networks take multiple filters over a single image, each one picking up a different signal. Conv nets moves the filter (e.g, a vertical-line-recognizing) over the actual pixels of the image, looking for matches. Each time a match is found, it is mapped onto a feature space particular to that visual element.</p><p>A conv network receives a normal colar image as a rectangular box whose width and height are measured by #pixels along those dimensions, and whose depth is three layers deep, one for each letter in RGB. Those depth layers are referred to as <strong>channels</strong>. For each pixel of an image, the intensity of R/G/B will be expressed by a number, and that number will be an element in one of the three, stacked 2D matrices, which together form the image volume. These numbers are the intial, raw, sensory features being fed into the conv net, and the ConvNets purpose is to find which of those numbers are significant signals that actually help it classify images more accurately.</p><p>Rather than focus on one pixel at a time, a conv net takes in square patches of pixels and passes them through a <strong>filter</strong>. That filter (also called a <strong>kernel</strong>) is also a square matrix with equal size to the patch, and the filter is to find patterns in the pixels. We are going to take the dot product of the filter with this patch of the image channel. If the two matrices have high values in the same positions, the dot product&#39;s output will be high. Ow, it will be low. In this way, a single value - the output of the dot product - can tell us whether the pixel pattern in the underlying image matches the pixel pattern expressed by our filter.</p><p>CNNs/DNNs exploit spatially-layers local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. A CNN/DNN is a sequence of multiple instances of four types of layers: a <strong>convolutional layer</strong> (CONV), <strong>pooling layer</strong>s (POOL), <strong>local response normalization layer</strong>s (LRN) and <strong>classifier layer</strong>s (CLASS), .</p><p><img src='_cnnlayers.gif' alt='_cnnlayers' /></p><h3><a name='header-c84' class='md-header-anchor '></a>1) Convolutional layer (CONV)</h3><p><!-- Instead of connecting every input pixel to every hidden neuron, we only make connections in small, localized regions of the input image. To be more precise, each neuron in the first hidden layer will be connected to a small region of the input neurons, e.g., a 5x5 region. --></p><p>From five to even several hundred CONV layers are commonly used in recent CNN models. Through the computation of each layer, a higher-level abstraction of the input data, called a <strong>feature map</strong>, is extracted to preserve essential yet unique info [6]. Generally, basic feature maps are first extracted, and then more complex feature maps will be obtained.</p><p>The feature map is obtained by repeated application of a function across small sub-regions of the entire image at layer <code>m</code>.
That region in the input image of layer <code>m</code> is called the <strong>local receptive field</strong> for the hidden neuron of layer <code>m+1</code>. It&#39;s a little window on the input pixels. Each connection learns a weight. And the hidden neuron learns an overall bias as well. We then slide the local receptive field across the entire input images. For each local receptive field, there is a different hidden neuron in the first hidden layer.
 <img src='_cnnhidden.png' alt='_cnnhidden' /></p><p>As for the above example, each hidden neuron thus has a bias and <code>5x5</code> weights connected to its local receptive field. </p><p><!-- The map from the input receptive field to the hidden neuron is called a **feature map**; and the weights and bias defining the feature map are often said to define a **kernel** or **filter**. More precisely, --></p><p>A <strong>filter</strong> is defined by <code>Kx x Ky</code> coefficients, which are learned and form the layer synaptic weights.The coefficients themselves form the <strong>kernel</strong>. Each conv layer slides <code>Nof</code> such filters through the whole input layer (by steps of <code>sx</code> and <code>sy</code>), resulting in as many (<code>Nof</code>) output neurons. E.g., if we have a <code>28x28</code> input image, and <code>5x5</code> local receptive fields (i.e., filter), and move the local receptive field by one pixel at a time (<code>sx=1</code>, <code>sy=1</code>), then there will be <code>24x24</code> (<code>Nof</code>) neurons in the hidden layer, each has <code>25=5x5</code> weights and a single bias. A complete convolution layer consists of several (<code>Nif</code>) different feature maps, which can detect <code>Nif</code> different kinds of features. Hence, the kernel is usually 3D, i.e., <code>Kx x Ky x Nif</code>.</p><p>In DNNs, the kernels usually have different synaptic values for each output neuron (at each <code>(x,y)</code> position), while in CNNs, the kernels are <em>shared</em> across all neurons of the same output feature map, which greatly reduces the #parameters involved in the network (<code>Kx x Ky x Nif x Nof x Nx x Ny</code> vs. <code>Kx x Ky x Nif x Nof</code>, where <code>Nx</code> and <code>Ny</code> are the input layer dimensions).</p><h3><a name='header-c105' class='md-header-anchor '></a>2) Pooling layers (POOLs)</h3><p>Pooling layers simplify the info in the output from the convolutional layer, and they are usually used immediately after convolutional layers. </p><p>In detail, a pooling layer takes each feature map output from the convolutional layer and prepares a condensed feature map. One common procedure for pooling is <em>max-pooling</em>, where a pooling unit simply outputs the max activation in the 2x2 input region:
 <img src='_cnnpooling.png' alt='_cnnpooling' />
After pooling we have 12x12 neurons.</p><p>Max-pooling can be thought as a way for the network to ask whether a given feature is found anywhere in a region of the image. It then throws away the exact positional info. The intuition is that once a feature has been found, its exact location isn&#39;t as important as its rough location relative to other features. Besides max-pooling, there is another common approach known as <em>L2 pooling</em>, which takes the square root the sum of squares of the activations in the 2x2 region.</p><p><u>Note: unlike a conv or a classifier layer, a pooling layer has no learned parameter, i.e., no synaptic weight</u>.</p><h3><a name='header-c120' class='md-header-anchor '></a>3) Local response normalization layers (LRNs)</h3><p>Local response normalization implements competition between neurons at the same location, but in different (neighbor) feature maps. LRNs are to mix multiple feature maps into one.</p><h3><a name='header-c123' class='md-header-anchor '></a>4) Classifier layers (CLASSs)</h3><p>The result or the sequence of CONV, POOL and LRN layers is then fed to one or multiple classifier layers. This layer is typically fully connected to its <code>Ni</code> inputs (and it has <code>No</code> outputs), and each connection carries a learned synaptic weight. </p><p>While the #inputs may be much lower than for other layers (due to the dimension reduction of pooling layers), they can account for a large share of all synaptic weights in the neural network due to their full connectivity. Multi-Layer perceptrons are frequently used as classifier layers, though other types of classifiers are used as well (e.g., multinomial logistic regression). The goal of these layers is naturally to correlate the different features extracted from the filtering, normalization and pooling steps and the output categories.</p><h2><a name='header-c132' class='md-header-anchor '></a>3. Research works</h2><p>The main limitation is the memory bandwidth requirements of two important layer types: conv layers with private kernels (used in DNNs) and classifier layers used in both CNNs and DNNs. For these types of layers, the total #required synapses can be massive, in Ms of parameters, or even tens or hundreds thereof. For an NPU processing 16 inputs of 16 output neurons (i.e., 256 synapses) per cycle, at 0.98GHz a peak bw of 467.30 GB/s would be necessary. Off-chip memory accesses increase the total energy cost by 10x.</p><p>The fundamental issue is the memory storage (for reuse) or bandwidth requirements (for fetching) of the synapses of two types of layers: conv layers with private kernels (the most frequent case in DNNs), and classifier layers (which are usually fully connected, and thus have lots of synapses).</p><ul><li>synapses are always stored close to the neurons which will use them, minimizing data movement, saving both time and energy</li><li>create an asymmetric arch where each node footprint is massively biased towards storage rather than computations</li><li>transfer neurons values rather than synapses values because the former are orders of magnitude fewer than the latter in the aforementioned layers, requiring comparatively little bw.</li><li>enable high internal bw by breaking down the local storage into many tiles.</li></ul><p>a read access to a 256b wide eDRAM array at 28nm consumes 0.0192nJ (50uA, 0.9V, 606MHz), while a 256b read access to a Micron DDR3 DRAM consumes 6.18nJ at 28nm.</p><p>The complexity comes from the need to simultaneously process hundreds of filters and channels in the high-dimensional convolutions, which involve a significant amount of data movement. </p><p>minimizes data movement energy by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations.</p><p>CONV layers account for over 90% of the overall operations and generate a large amount of data movement. </p><ul><li>Data handling. Naively reading inputs for all MACs directly from DRAM requires high bw and incurs high energy consumption; second, a significant amount of immediate data, i.e., partial sums, are generated by the parallel MACs simultaneously, which poses storage pressure and consumes additional memory R/W energy if not accessed, i.e., accumulated, immediately.</li></ul><h3><a name='header-c162' class='md-header-anchor '></a>XNOR-net</h3><p>Find the best approximations of the convolutions using binary operations. Comparable accuracy, but significantly less memory and fewer FP operations.</p><p>L-layer CNN: <I, W, *></p><ul><li><strong>Binary-Weight-Networks</strong>: approximate all weight values with binary values; convolutions can be estimated by only addition and substraction (without multiplication).<br/>
<code>W = aB, I * W = (I?B)a</code><br/>
optimal estimation of a binary weight filter can be simply achieved by taking the sign of weight values; optimal scaling factor is the average of absolute weight values.<br/>
Once the training finished, there is no need to keep the real-value weights.</li><li><strong>XNOR-Networks</strong>: both the weights and the inputs to the conv and fully connected layers are approximated with binary values. The convolutions can be estimated by XNOR and bitcounting operations. <br/>
<code>I * W = (sign(I) ? sign(W)) ! Ka</code></li></ul><h3><a name='header-c178' class='md-header-anchor '></a>Deep compression</h3><p>Reduces the storage and energy required to run inference on large networks so they can be deployed on mobile devices.</p><h2><a name='header-c181' class='md-header-anchor '></a>References</h2><ol><li>Deep learning, <a href='http://neuralnetworksanddeeplearning.com/chap6.html'>http://neuralnetworksanddeeplearning.com/chap6.html</a></li><li>DaDianNao: A machine-learning supercomputer, <a href='http://pages.saclay.inria.fr/olivier.temam/files/eval/supercomputer.pdf'>http://pages.saclay.inria.fr/olivier.temam/files/eval/supercomputer.pdf</a></li><li>Neural networks, <a href='http://natureofcode.com/book/chapter-10-neural-networks/'>http://natureofcode.com/book/chapter-10-neural-networks/</a></li><li>Convolutional neural networks CNNs, <a href='http://recognize-speech.com/acoustic-model/knn/comparing-different-architectures/convolutional-neural-networks-cnns'>http://recognize-speech.com/acoustic-model/knn/comparing-different-architectures/convolutional-neural-networks-cnns</a></li><li>Convolutional Neural Networks (LeNet), <a href='http://deeplearning.net/tutorial/lenet.html'>http://deeplearning.net/tutorial/lenet.html</a></li><li>Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks, ISCA&#39;2016</li><li>ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars, ISCA&#39;2016</li><li>Convolutional neural network, <a href='http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/'>http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/</a></li><li>Convolutional networks, <a href='http://deeplearning4j.org/convolutionalnets'>http://deeplearning4j.org/convolutionalnets</a></li><li>Convolutional neural networks (CNNs/ConvNets), <a href='https://cs231n.github.io/convolutional-networks/'>https://cs231n.github.io/convolutional-networks/</a></li></ol><p></p><p></p><p></p><p></p></div>
</body>
</html>